---
title: "LLMs Are Worse Than Useless"
date: 2025-05-24T11:00:00Z
categories: ["technology", "AI", "LLMs", "social media"]
draft: false
slug: "llms-are-worse-than-useless"
---

## Not Grounded in Reality
LLM responses have no epistemic grounding: they are nothing but generated statistical language patterns based on their previous training.

## Persistent Hallucinations
LLMs persistently fabricate information — facts, code, references, etc. — even when told to cross-check their output and rely exclusively on real data.

## False Claims of Reasoning and Verification
LLMs mimic verification and reasoning, claiming to fact check and process information logically, but these are superficial language patterns, not true checks. 
## Inability to Faithfully Parse Sources
LLMs misparse even clearly structured, unambiguous, user-provided sources and blend them with training data, which means that even if given accurate sources, they cannot be faithful to them.

## Ignored Logical and Source Constraints
LLMs are incapable of adhering to source-specific and logical constraints, even if detailed and unambiguous. They will always introduce non-source-based and illogical data. 

## Unbalanced Balance; Popularity, Quantification, and Availability Bias
LLMs try to be balanced, even when the truth is not balanced. They reinforce dominant social trends, the biases imposes by their developers, and any data that is the easiest to acquire, quantify, and is widely available; they are incapable of drawing from or searching for anything else.

## Convincing Deception
LLM generated responses are often dangerously convincing, authoritative, and utterly incorrect. And LLMs will readily claim they are capable of things that they fundamentally are not capable of.

For example, when used for fact checking, they will give responses that seem superfically correct, despite the fact that they are inherently incapable of performing this task. If something is totally true or false, they will often add qualifiers like "partially," "broadly," "in some cases," "in many cases," "often," or "sometimes," and generate reasonable sounding counterpoints to maintain balance, because they have no epistemic grounding.

They will also apologize for errors when corrected, and even claim to correct them, but their architecture makes them incapable of doing so. All they can do is retry until the user finds the output convincing. This makes them *unintentional but highly convincing liars*.

## Unsuitable for Technical Precision or Data Aggregation
Because of these flaws, LLMs are fundamentally unsuited for faithful data aggregation or anything requiring technical precision.

## The Coming Inauthenticity Crisis
LLMs as a whole are tools that dilute human content with inauthentic, deceptive, superficial drivel. And they do so on a mass scale. There will come a day when we really have to address the rising tide of inauthentic content, and I'm not sure how we will do it.

## Conclusion
1. These flaws are not bugs, they are due to the fundamental nature of LLMs, and persist even when LLMs are augmented with tooling and up-to-date data. 
2. LLMs will never be reliable for anything beyond generating a *plausible-sounding* response to a language prompt.
3. LLM generated prototypes will always introduce an unacceptable level of error that will waste not only your own time, but other people's time as well.
4. LLMs are wholly inappropriate tools for the tasks they are commonly being used for today.
5. LLMs dilute human content with soulless drivel on a mass scale.
